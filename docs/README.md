
## Problem statement

We try to solve a stochastic dynamic programming problem
in inventory control, with the following DP equation:
$$
V _i (x)= \max _{z,q} [\mathbf {E} ~ \textrm {cost} 
(x,z,q,D) + \alpha \mathbf {E} ~ V _{i+1} (\textrm {dynamic} (x,z,q,D))]
$$

where \\(x \in S= [0, k-1] ^m\\), that is, each state
is a m-dimension vector \\(x= (x _1, ..., x _m) \\), 
with each element in the range \\([0,k-1]\\).
An action is a two-dimension vector \\( (z,q)\\).
\\(z\\) stands for depletion quantity, \\(q\\) order quantity.
\\(V _i (x)\\) is called the value of state \\(x\\)
in period \\(i\\).

Here is the story: The firm holds inventory with different
expiry dates, for instance \\(x=(2,4,3)\\) means there is 
2 units of inventory that will expire in one period, 4 units
that will expire in two periods, and 3 units that will expire
in three periods. The manager standing at the beginning of the period faces two decisions: how much to deplete (z)
 and how much to order (q). The depletion is to reduce the holding cost as well as reduce potential expiry items,
and the order is to fulfill forthcoming demands. The timeline is as follows: At the beginning of each period,
the manager decides the depletion quantity z and order quantity q, then demand realizes, and remaining inventory
carried over to the next period (those unsold inventory with one period to expire go outdated). For example,
if \\( (z,q)=(2,3), D=4\\), then 
\\( x=(4,1,2)\rightarrow (2,1,2) (\textrm{depleted by 2 units}) \rightarrow (2,1,2,3)  (\textrm{3 units of order added })
\rightarrow (0,0,1,3) (\textrm{demand realized}) \rightarrow  (0,1,3) = \textrm{dynamic} (x, z,q, D).\\)
  

The boundary condition \\(V _{T} (\cdot)\\) is given, so 
we can compute \\(V _0 (\cdot)\\) in a bottom-up way:
first compute \\(V _{T-1} (\cdot)\\) based on \\(V _{T} (\cdot)\\)
and the DP equation, then \\(V _{T-2} (\cdot)\\) based on \\(V _{T-1} (\cdot)\\),etc.

A policy \\(f\\) specifies the action in each period
based on the current state, and we have the following 
equation
$$
V _{f,i} (x)= \mathbf {E} ~ \textrm {cost} 
(x,z _f,q _f,D) + \alpha \mathbf {E} ~ V _{f, i+1} (\textrm {dynamic} (x,z _f,q _f,D))
$$

where \\( (z _f, q _f) \\) is the action taken under
policy \\(f\\), when the state is \\(x\\) and period 
\\(i\\).

Our objective is to compute \\(V _0 (x)\\) and 
\\( V _{f,0} (x)\\) for some specified policy \\(f\\), and 
expect that the relative difference is small when some
model parameters scaled up. 

## Algorithm description

We have two parts: first compute \\(V _0 (x)\\) then
\\( V _{f,0} (x)\\).

In the first part, we use an array **Valuevec** to store
the value of each state in \\(S= [0, k-1] ^m\\), so
the length of **Valuevec** is \\(k ^m\\). 

Initially
set **Valuevec** \\( =V _T (\cdot)\\).  Then at each iteration we compute \\(V _{i-1} (\cdot)\\) from \\(V _{i} (\cdot)\\), which is stored in **Valuevec**,  and update **Valuevec** with \\(V _{i-1} (\cdot)\\). After \\(T\\) iterations, we obtain \\(V _0 (\cdot)\\) stored in **Valuevec**.

The second part is a straightforward recursive computation, thus omitted.

### list of notations and variables

- **m** dimension of state space
- **k** max number on each dimension of state
- **T** number of periods
- **n_sample** sample size used to approximate expectation
- **drate,h,r,c,theta,s,alpha,maxhold** other model parameters
- **Valuevec**
  an array storing the value of all states.
- **temp**
  an intermediate array storing the result at each iteration, then passed on to **Valuevec**.
- **ind**
  state indicator array, "true" stands for already
  calculated, "false" not calculated.
- **Demands**
  sample paths generated by **genDemand()**
- **genDemand** generate sample paths of each period
- **code, decode** link each state (a high-dim array) to
its subscript in **Valuevec**.
for example, when \\(k=10, m=3\\), then each state 
\\( (a,b,c)\\) is stored in **Valuevec** with subscript
 \\(100a+10b+c\\). So code\\( (2,3,4)= 234\\), decode
\\( (436)= (4,3,6). \\)
- **cost_to_go** cost function in the DP equation
- **dep**  the remaining profile of state \\(x\\) 
after depleting \\(z\\) units 
- **dep_one** the remaining profile after depleting one unit
- **dynamic** the evolution dynamic of state from current
period to next period
- **obj**  the objective value function appearing in 
the DP equation, which we try to maximize on.
- **optq** the optimal order quantity of state \\(x\\),
  after depletion is committed.
- **wrapobj** the wrapped version of **obj**, only containing decision variable \\(z\\).
- **optz** the optimal depletion quantity \\(z\\) that maximizes
**wrapobj**
- **fillvalue!**  the procedure of filling **temp** 
- **premainprog!** calculate some state values on the local machine.
- **mainprog!** calculate values of those states distributed to machine with label **jobno**.

### State space cut and optimization
Our algorithm has \\(T\\) big iterations, which is processed sequentially, thus cannot be paralleled in this level. In each iteration, however, we have to compute the value
of each  state using the DP equation,  equivalent to solving \\(k ^m\\) subproblems, which can be paralleled.

Also, we can use the tree structure of state space to reduce computation. **dep_one** shows how the tree is formed. For each state x, we can deplete one by one until it's empty, thus forming a path, for example:
\\( (1,3,1)\rightarrow (0,3,1) \rightarrow (0,2,1) \rightarrow (0,1,1) \rightarrow (0,0,1) \rightarrow (0,0,0).\\)
Each state regarded as a node, will eventually go to the root node
\\( (0,...,0)\\). Thus we formed a tree for our state space.

Our observation is that if one node is computed, then all its son nodes (the path to the root node) needs not be computed (detail needs here). Thus we can just compute 
all leaf nodes to obtain the value of all nodes. 
Also, different nodes have common son nodes, which should be utilized to optimize the algorithm.

