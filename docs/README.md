<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


## Problem statement

We try to solve a stochastic dynamic programming problem
in inventory control, with the following DP equation:
$$
V _i (x)= \max _{z,q} [\mathbf {E} ~ \textrm {cost} 
(x,z,q,D) + \alpha \mathbf {E} ~ V _{i+1} (\textrm {dynamic} (x,z,q,D))]
$$

where \\(x \in S= [0, k-1] ^m\\), that is, each state
is a m-dimension vector \\(x= (x _1, ..., x _m) \\), 
with each element in the range \\([0,k-1]\\).
An action is a two-dimension vector \\( (z,q)\\).
\\(z\\) stands for depletion quantity, \\(q\\) order quantity.
\\(V _i (x)\\) is called the value of state \\(x\\)
in period \\(i\\)

The boundary condition \\(V _{T} (\cdot)\\) is given, so 
we can compute \\(V _0 (\cdot)\\) in a bottom-up way:
first compute \\(V _{T-1} (\cdot)\\) based on \\(V _{T} (\cdot)\\)
and the DP equation, then \\(V _{T-2} (\cdot)\\) based on \\(V _{T-1} (\cdot)\\),etc.

A policy \\(f\\) specifies the action in each period
based on the current state, and we have the following 
equation
$$
V _{f,i} (x)= \mathbf {E} ~ \textrm {cost} 
(x,z _f,q _f,D) + \alpha \mathbf {E} ~ V _{f, i+1} (\textrm {dynamic} (x,z _f,q _f,D))
$$

where \\( (z _f, q _f) \\) is the action taken under
policy \\(f\\), when the state is \\(x\\) and period 
\\(i\\).

Our objective is to compute \\(V _0 (x)\\) and 
\\( V _{f,0} (x)\\) for some specified policy \\(f\\), and 
expect that the relative difference is small when some
model parameters scaled up. 

## Algorithm description

We have two parts: first compute \\(V _0 (x)\\) then
\\( V _{f,0} (x)\\).

In the first part, we use an array **Valuevec** to store
the value of each state in \\(S= [0, k-1] ^m\\), so
the length of **Valuevec** is \\(k ^m\\). 

Initially
set **Valuevec** \\( =V _T (\cdot)\\).  Then at each iteration we compute \\(V _{i-1} (\cdot)\\) from \\(V _{i} (\cdot)\\), which is stored in **Valuevec**,  and update **Valuevec** with \\(V _{i-1} (\cdot)\\). After \\(T\\) iterations, we obtain \\(V _0 (\cdot)\\) stored in **Valuevec**.

The second part is a straightforward recursive computation, thus omitted.

### list of notations and variables

- **m** dimension of state space
- **k** max number on each dimension of state
- **T** number of periods
- **n_sample** sample size used to approximate expectation
- **drate,h,r,c,theta,s,alpha,maxhold** other model parameters
- **Valuevec**
  an array storing the value of all states.
- **temp**
  an intermediate array storing the result at each iteration, then passed on to **Valuevec**.
- **ind**
  state indicator array, "true" stands for already
  calculated, "false" not calculated.
- **Demands**
  sample paths generated by **genDemand()**
- **genDemand** generate sample paths of each period
- **code, decode** link each state (a high-dim array) to
its subscript in **Valuevec** 
- **cost_to_go** cost function in the DP equation
- **dep**  the remaining profile of state \\(x\\) 
after depleting \\(z\\) units 
- **dep_one** the remaining profile after depleting one unit
- **dynamic** the evolution dynamic of state from current
period to next period
- **obj**  the objective value function appearing in 
the DP equation, which we try to maximize on.
- **optq** the optimal order quantity of state \\(x\\),
  after depletion is committed.
- **wrapobj** the wrapped version of **obj**, only containing decision variable \\(z\\).
- **optz** the optimal depletion quantity \\(z\\) that maximizes
**wrapobj**
- **fillvalue!**  the procedure of filling **temp** 
- **premainprog!** calculate some state values on the local machine.
- **mainprog!** calculate values of those states distributed to machine with label **jobno**.

